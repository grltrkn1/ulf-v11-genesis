#!/usr/bin/env python3
"""
analyze_results.py

ULF v11 Genesis Project â€” The Analysis Engine
================================================

This script is the "sensory cortex" of the ULF system. It ingests the raw
experimental data (results.csv) generated by the simulation engine and transforms
it into a structured, meaningful CSIReport (analysis_summary.txt) and a series
of diagnostic plots.

It computes the core metrics of homeostatic resilience (PI, ECI, DLI), performs
statistical tests, and provides the quantitative evidence that the SEI needs to
drive the Autonomous Scientific Cycle.

Author: ULF Genesis Core Team
License: MIT
"""

import sys
import argparse
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# --------------------------------------------------------------------------
# 1. CORE STATISTICAL AND METRIC FUNCTIONS
# --------------------------------------------------------------------------

def calculate_csi_from_rewards(rewards_metric, rewards_symbolic):
    """
    A simplified CSI calculation for this context. We define the sub-indices
    based on the comparative performance of the Metric vs. Symbolic planners.
    """
    # Persistence Index (PI): The ability to avoid catastrophic failure.
    # We'll measure this as the success rate (reward > 0).
    pi_metric = np.mean(rewards_metric > 0)
    pi_symbolic = np.mean(rewards_symbolic > 0)
    
    # Error Correction Index (ECI): The magnitude of the improvement.
    # We'll use the mean reward difference as a proxy.
    mean_diff = np.mean(rewards_metric - rewards_symbolic)
    # Normalize to a [0, 1] scale. Assume max possible diff is ~200 (GOAL_REWARD - TRAP_PENALTY).
    eci = max(0, min(1, mean_diff / 200.0))

    # Durable Learning Index (DLI): The reliability of the improvement.
    # We'll use 1 minus the p-value of the difference as a proxy for confidence.
    _, p_value = stats.ttest_rel(rewards_metric, rewards_symbolic)
    dli = 1.0 - p_value

    # Overall CSI: A weighted average.
    # We weight ECI and DLI higher as they represent improvement and confidence.
    csi = (0.2 * pi_metric) + (0.4 * eci) + (0.4 * dli)
    
    return {
        "persistence_index": pi_metric,
        "error_correction_index": eci,
        "durable_learning_index": dli,
        "overall_csi": csi
    }

def cohens_d_paired(a, b):
    """Calculates Cohen's d for paired samples."""
    diff = np.asarray(a) - np.asarray(b)
    return np.mean(diff) / (np.std(diff, ddof=1) + 1e-9)

def bootstrap_ci(data, stat_func, n_boot=2000, alpha=0.05):
    """Computes a bootstrap confidence interval for a given statistic."""
    n = len(data)
    if n == 0: return (0, 0)
    boot_stats = [stat_func(np.random.choice(data, n, replace=True)) for _ in range(n_boot)]
    lower_bound = np.percentile(boot_stats, 100 * (alpha / 2.0))
    upper_bound = np.percentile(boot_stats, 100 * (1.0 - alpha / 2.0))
    return (lower_bound, upper_bound)

# --------------------------------------------------------------------------
# 2. ANALYSIS AND REPORTING
# --------------------------------------------------------------------------

def analyze_results(df: pd.DataFrame):
    """
    Performs a full analysis of the experimental results and returns a
    dictionary of key findings.
    """
    if 'reward_metric' not in df.columns or 'reward_symbolic' not in df.columns:
        raise ValueError("Input CSV must contain 'reward_metric' and 'reward_symbolic' columns.")

    r_metric = df['reward_metric'].values
    r_symbolic = df['reward_symbolic'].values

    # --- Core Statistical Tests ---
    mean_metric = np.mean(r_metric)
    mean_symbolic = np.mean(r_symbolic)
    mean_diff = mean_metric - mean_symbolic
    
    t_test_result = stats.ttest_rel(r_metric, r_symbolic)
    try:
        wilcoxon_result = stats.wilcoxon(r_metric - r_symbolic)
    except ValueError: # Occurs if all differences are zero
        wilcoxon_result = None

    cohen_d = cohens_d_paired(r_metric, r_symbolic)
    ci_lower, ci_upper = bootstrap_ci(r_metric - r_symbolic, np.mean)
    
    # --- CSI Calculation ---
    csi_metrics = calculate_csi_from_rewards(r_metric, r_symbolic)

    analysis = {
        "n_episodes": len(df),
        "mean_reward_metric": mean_metric,
        "mean_reward_symbolic": mean_symbolic,
        "mean_difference": mean_diff,
        "t_statistic": t_test_result.statistic,
        "p_value": t_test_result.pvalue,
        "wilcoxon_statistic": wilcoxon_result.statistic if wilcoxon_result else "N/A",
        "wilcoxon_p_value": wilcoxon_result.pvalue if wilcoxon_result else "N/A",
        "cohens_d": cohen_d,
        "bootstrap_ci_95": [ci_lower, ci_upper],
        **csi_metrics
    }
    return analysis

def generate_report(analysis: Dict, output_path: str):
    """Writes the analysis results to a human-readable text file."""
    with open(output_path, 'w') as f:
        f.write("ULF v11 - Autonomous Scientific Cycle - Analysis Report\n")
        f.write("="*60 + "\n\n")
        f.write(f"Number of Episodes: {analysis['n_episodes']}\n\n")
        
        f.write("--- Performance Summary ---\n")
        f.write(f"Mean Reward (Metric ATL):     {analysis['mean_reward_metric']:.4f}\n")
        f.write(f"Mean Reward (Symbolic ATL):   {analysis['mean_reward_symbolic']:.4f}\n")
        f.write(f"Mean Improvement (Metric - Symbolic): {analysis['mean_difference']:.4f}\n\n")

        f.write("--- Statistical Significance ---\n")
        f.write(f"Paired t-test (t-statistic): {analysis['t_statistic']:.4f}\n")
        f.write(f"Paired t-test (p-value):     {analysis['p_value']:.6f}\n")
        f.write(f"Wilcoxon signed-rank (p-value): {analysis['wilcoxon_p_value']:.6f}\n")
        f.write(f"Cohen's d (Effect Size):     {analysis['cohens_d']:.4f}\n")
        f.write(f"95% Bootstrap CI for Mean Diff: ({analysis['bootstrap_ci_95'][0]:.4f}, {analysis['bootstrap_ci_95'][1]:.4f})\n\n")

        f.write("--- Cluster Stability Index (CSI) ---\n")
        f.write(f"Persistence Index (PI):       {analysis['persistence_index']:.4f}\n")
        f.write(f"Error Correction Index (ECI): {analysis['error_correction_index']:.4f}\n")
        f.write(f"Durable Learning Index (DLI): {analysis['durable_learning_index']:.4f}\n")
        f.write(f"Overall CSI Score:            {analysis['overall_csi']:.4f}\n\n")

        f.write("--- Conclusion ---\n")
        if analysis['p_value'] < 0.01 and analysis['cohens_d'] > 0.8:
            f.write("Result: Strong evidence of Metric ATL superiority. The structural hypothesis is validated.\n")
        elif analysis['p_value'] < 0.05:
            f.write("Result: Moderate evidence of Metric ATL superiority. Further testing may be warranted.\n")
        else:
            f.write("Result: No significant evidence of Metric ATL superiority. The hypothesis is not supported by this data.\n")
    print(f"Analysis summary saved to {output_path}")

def generate_plots(df: pd.DataFrame, output_dir: str):
    """Generates and saves diagnostic plots."""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # 1. Reward Distribution Plot
    plt.figure(figsize=(10, 6))
    plt.hist(df['reward_metric'], bins=30, alpha=0.7, label='Metric ATL', color='blue')
    plt.hist(df['reward_symbolic'], bins=30, alpha=0.7, label='Symbolic ATL', color='orange')
    plt.title('Distribution of Episode Rewards')
    plt.xlabel('Reward')
    plt.ylabel('Frequency')
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.savefig(os.path.join(output_dir, 'reward_distribution.png'))
    plt.close()

    # 2. Paired Reward Scatter Plot
    plt.figure(figsize=(8, 8))
    plt.scatter(df['reward_symbolic'], df['reward_metric'], alpha=0.5)
    lims = [
        min(df['reward_symbolic'].min(), df['reward_metric'].min()),
        max(df['reward_symbolic'].max(), df['reward_metric'].max())
    ]
    plt.plot(lims, lims, 'r--', alpha=0.75, label='y=x (No Difference)')
    plt.title('Paired Episode Rewards: Metric vs. Symbolic')
    plt.xlabel('Symbolic ATL Reward')
    plt.ylabel('Metric ATL Reward')
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend()
    plt.savefig(os.path.join(output_dir, 'reward_comparison_scatter.png'))
    plt.close()

    print(f"Diagnostic plots saved to {output_dir}/")

# --------------------------------------------------------------------------
# 3. MAIN ENTRY POINT
# --------------------------------------------------------------------------

def main():
    """Main function to run the analysis."""
    parser = argparse.ArgumentParser(description="ULF v11 - Analysis Engine")
    parser.add_argument("results_csv", help="Path to the results.csv file from the simulation.")
    parser.add_argument("--output-file", default="analysis_summary.txt", help="Path to save the summary text file.")
    parser.add_argument("--plot-dir", default="analysis_plots", help="Directory to save diagnostic plots.")
    args = parser.parse_args()

    print("--- ULF v11 Genesis: Analysis Engine ---")
    
    try:
        df = pd.read_csv(args.results_csv)
    except FileNotFoundError:
        print(f"Error: Input file not found at '{args.results_csv}'")
        sys.exit(1)

    # Perform analysis
    analysis_data = analyze_results(df)
    
    # Generate report and plots
    generate_report(analysis_data, args.output_file)
    generate_plots(df, args.plot_dir)

    print("Analysis complete.")

if __name__ == "__main__":
    main()
